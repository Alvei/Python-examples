{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7 - Ensembles\n",
    "This whole lesson (on ensembles) is about how we can combine (or ensemble) the models you have already seen in a way that makes the combination of these models better at predicting than the individual models.\n",
    "\n",
    "Commonly the \"weak\" learners you use are decision trees. In fact the default for most ensemble methods is a decision tree in sklearn. However, you can change this value to any of the models you have seen so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Would We Want to Ensemble Learners Together?\n",
    "There are two competing variables in finding a well fitting machine learning model: Bias and Variance. It is common in interviews for you to be asked about this topic and how it pertains to different modeling techniques. As a first pass, the wikipedia is quite useful. However, I will give you my perspective and examples:\n",
    "\n",
    "**Bias**: When a model has high bias, this means that means it doesn't do a good job of bending to the data. An example of an algorithm that usually has high bias is linear regression. Even with completely different datasets, we end up with the same line fit to the data. When models have high bias, this is bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variance**: When a model has high variance, this means that it changes drastically to meet the needs of every point in our dataset. Linear models like the one above has low variance, but high bias. An example of an algorithm that tends to have high variance and low bias is a decision tree (especially decision trees with no early stopping parameters). A decision tree, as a high variance algorithm, will attempt to split every point into its own branch if possible. This is a trait of high variance, low bias algorithms - they are extremely flexible to fit exactly whatever data they see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By combining algorithms, we can often build models that perform better by meeting in the middle in terms of bias and variance. There are some other tactics that are used to combine algorithms in ways that help them perform better as well. These ideas are based on minimizing bias and variance based on mathematical theories, like the central limit theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Randomness Into Ensembles\n",
    "Another method that is used to improve ensemble methods is to introduce randomness into high variance algorithms before they are ensembled together. The introduction of randomness combats the tendency of these algorithms to overfit (or fit directly to the data available). There are two main ways that randomness is introduced:\n",
    "\n",
    "**Bootstrap the data** - that is, sampling the data with replacement and fitting your algorithm to the sampled data.\n",
    "\n",
    "**Subset the features** - in each split of a decision tree or with each algorithm used in an ensemble, only a subset of the total possible features are used.\n",
    "\n",
    "In fact, these are the two random components used in the next algorithm you are going to see called `random forests`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additional Resources on Boosting**\n",
    "\n",
    "[The original paper](https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf) - A link to the original paper on boosting by Yoav Freund and Robert E. Schapire.\n",
    "\n",
    "[An explanation about why boosting is so important](https://medium.com/kaggle-blog) - A great article on boosting by a Kaggle master, Ben Gorman.\n",
    "\n",
    "[A useful Quora post](https://www.quora.com/What-is-an-intuitive-explanation-of-Gradient-Boosting) - A number of useful explanations about boosting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$weight = \\ln (\\frac{accuracy}{1-accuracy})$ =  $ \\ln (\\frac{number   of correct}{number   of incorrect})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost in sklearn\n",
    "Building an AdaBoost model in sklearn is no different than building any other model. You can use scikit-learn's `AdaBoostClassifier` class. This class provides the functions to define and fit the model to your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "> model = AdaBoostClassifier()\n",
    "\n",
    "> model.fit(x_train, y_train)\n",
    "\n",
    "> model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the model variable is a decision tree model that has been fitted to the data `x_train` and `y_train`. The functions fit and predict work exactly as before.\n",
    "\n",
    "### Hyperparameters\n",
    "When we define the model, we can specify the hyperparameters. In practice, the most common ones are\n",
    "\n",
    "`base_estimato`r: The model utilized for the weak learners (Warning: Don't forget to import the model that you decide to use for the weak learner).\n",
    "`n_estimators`: The maximum number of weak learners used.\n",
    "\n",
    "For example, here we define a model which uses decision trees of max_depth 2 as the weak learners, and it allows a maximum of 4 of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "> model = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth=2), n_estimators = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "In this lesson, you learned about a number of techniques used in ensemble methods. Before looking at the techniques, you saw that there are two variables with tradeoffs Bias and Variance.\n",
    "\n",
    "High Bias, Low Variance models tend to underfit data, as they are not flexible. Linear models fall into this category of models.\n",
    "\n",
    "High Variance, Low Bias models tend to overfit data, as they are too flexible. Decision trees fall into this category of models.\n",
    "\n",
    "### Ensemble Models\n",
    "In order to find a way to optimize for both variance and bias, we have ensemble methods. Ensemble methods have become some of the most popular methods used to compete in competitions on Kaggle and used in industry across applications.\n",
    "\n",
    "There were two randomization techniques you saw to combat overfitting:\n",
    "\n",
    "1. Bootstrap the data - that is, sampling the data with replacement and fitting your algorithm and fitting your algorithm to the sampled data.\n",
    "\n",
    "2. Subset the features - in each split of a decision tree or with each algorithm used an ensemble only a subset of the total possible features are used.\n",
    "\n",
    "### Techniques\n",
    "You saw a number of ensemble methods in this lesson including:\n",
    "\n",
    "- [BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier)\n",
    "- [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)\n",
    "- [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier)\n",
    "- Another really useful guide for ensemble methods can be found in the [documentation here](https://scikit-learn.org/stable/modules/ensemble.html). These methods can also all be extended to regression problems, not just classification.\n",
    "\n",
    "### Additional Resources\n",
    "Additionally, here are some great resources on AdaBoost if you'd like to learn some more!\n",
    "\n",
    "Here is the original [paper](https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf) from Freund and Schapire.\n",
    "\n",
    "A follow-up [paper](https://cseweb.ucsd.edu/~yfreund/papers/boostingexperiments.pdf) from the same authors regarding several experiments with Adaboost.\n",
    "\n",
    "A great [tutorial](http://rob.schapire.net/papers/explaining-adaboost.pdf) by Schapire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
