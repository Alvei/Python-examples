{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of references for project 3\n",
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [The Comprehensive Guide for Feature Engineering](https://adataanalyst.com/machine-learning/comprehensive-guide-feature-engineering/_)\n",
    "* [Fundamental Techniques of Feature Engineering for Machine Learning](https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114)\n",
    "* [Best Practices for Feature Engineering](https://elitedatascience.com/feature-engineering-best-practices)\n",
    "* [Data Cleaning with Python and Pandas](https://towardsdatascience.com/data-cleaning-with-python-and-pandas-detecting-missing-values-3e9c6ebcf78b)\n",
    "* [3 steps to a clean dataset with Pandas](https://towardsdatascience.com/3-steps-to-a-clean-dataset-with-pandas-2b80ef0c81ae)\n",
    "* [A complete and comprehensive guide: Preprocessing with sklearn](https://towardsdatascience.com/preprocessing-with-sklearn-a-complete-and-comprehensive-guide-670cb98fcfb9)\n",
    "* [A nice discussion about Fit_transform() vs. transform()](https://datascience.stackexchange.com/questions/12321/whats-the-difference-between-fit-and-fit-transform-in-scikit-learn-models#:~:text=The%20transform%20function%20applies%20the,or%20in%20a%20single%20step.)\n",
    "\n",
    "### Missing Values\n",
    "* [Handling Missing Values](https://www.kaggle.com/dansbecker/handling-missing-values/notebook)\n",
    "* [How to handle missing NaNs for machine learning in python](https://stackoverflow.com/questions/27824954/how-to-handle-missing-nans-for-machine-learning-in-python)\n",
    "\n",
    "There is no perfect way to compensate for the missing values in a dataset. Each strategy can perform better for certain datasets and missing data types but may perform much worse on other types of datasets. There are some set rules to decide which strategy to use for particular types of missing values, but beyond that, you should experiment and check which model works best for your dataset. Below are some resources to know about Popular strategies to statistically impute missing values in a dataset.\n",
    "\n",
    "* [[6 Different Ways to Compensate for Missing Values In a Dataset (Data Imputation with examples)](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779)\n",
    "\n",
    "1) Do Nothing\n",
    "2) Imputation Using (Mean/Median) Values\n",
    "3) Imputation Using (Most Frequent) or (Zero/Constant) Values\n",
    "4) Imputation Using k-NN\n",
    "5) Imputation Using Multivariate Imputation by Chained Equation (MICE)\n",
    "6) Imputation Using Deep Learning (Datawig)\n",
    "\n",
    "## Scaling\n",
    "* [Why, How and When to Scale your Features](https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e)\n",
    "* [Scale, Standardize, or Normalize with Scikit-Learn](https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02)\n",
    "\n",
    "I would also suggest using the [inverse_transform()](https://stackoverflow.com/questions/49885007/how-to-use-scikit-learn-inverse-transform-with-new-values) method of the PCA and StandardScaler objects to transform [centroids](https://stackoverflow.com/questions/36638776/how-to-determine-the-centroid-of-pca) back to the original data space and interpret the retrieved values directly. With StandardScaler, strongly negative values suggest lower values on the original scale and positive values suggest higher values on the original scale. Just make sure to identify the split-off data for both the general data and the customer data as its own cluster when computing the proportion of data in each cluster.\n",
    "\n",
    "### Encoding\n",
    "* [Encoding Categorical Features](https://towardsdatascience.com/encoding-categorical-features-21a2651a065c)\n",
    "* [Guide to Encoding Categorical Values in Python](https://pbpython.com/categorical-encoding.html)\n",
    "\n",
    "### Mixed attributes\n",
    "* [Best way to classify datasets with mixed types of attributes](https://datascience.stackexchange.com/questions/418/best-way-to-classify-datasets-with-mixed-types-of-attributes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "Among python scientific computation libraries, it is found Pandas to be the most useful for data science operations. Pandas, along with Scikit-learn provides almost the entire stack needed by a data scientist. Below are some article focuses on providing ways for data manipulation in Python together with some tips & tricks which will allow you to work faster.\n",
    "\n",
    "* [One-stop Guide to Data Manipulation in Python](https://medium.com/analytics-vidhya/python-data-manipulation-fb86d0cdd028)\n",
    "* [12 Useful Pandas Techniques in Python for Data Manipulation](https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/)\n",
    "* [5 Elegant Python Pandas Functions](https://towardsdatascience.com/5-elegant-python-pandas-functions-a4bf395ebef4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "* [Interpret the key results for Principal Components Analysis](https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/multivariate/how-to/principal-components/interpret-the-results/key-results/)\n",
    "* [Interpretation of the Principal Components](https://online.stat.psu.edu/stat505/node/54/)\n",
    "* [Principal Component Analysis explained](https://www.kaggle.com/nirajvermafcb/principal-component-analysis-explained)\n",
    "* [Principal Component Analysis](https://medium.com/maheshkkumar/principal-component-analysis-2d11043ff324)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "* [Determine the optimal number of clusters for k-means clustering](https://bl.ocks.org/rpgove/0060ff3b656618e9136b)\n",
    "* [kmeans elbow method](https://pythonprogramminglanguage.com/kmeans-elbow-method/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ploting\n",
    "* [Seaborn Tutorial: Count Plots](https://www.kaggle.com/slamnz/seaborn-tutorial-count-plots)\n",
    "* [Data Visualization using Seaborn](https://towardsdatascience.com/data-visualization-using-seaborn-fc24db95a850)\n",
    "* [Matplotlib Subplot](https://pythonspot.com/matplotlib-subplot/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation\n",
    "\n",
    "* [Find Your Best Customers with Customer Segmentation in Python](https://datascienceplus.com/find-your-best-customers-with-customer-segmentation-in-python/)\n",
    "* [A Quick Look at Market Segmentation](https://medium.com/@staceyferreira/a-look-at-customer-segmentation-43e053a8cef1)\n",
    "* [How to use Customer Segmentation To Learn](https://medium.com/@kristenkehrer/how-to-use-customer-segmentation-to-learn-f49e82b9a959)\n",
    "* [Customer Segmentation - SlideShare](https://www.slideshare.net/soaresc/customer-segmentation-6010726) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
