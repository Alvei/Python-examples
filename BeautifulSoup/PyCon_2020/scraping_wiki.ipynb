{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "imfanf17fEkM"
   },
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "<h1><TT>It's Officially Legal so Let's Scrape the Web</TT></h1>\n",
    "<br>\n",
    "Kimberly Fessel  \n",
    "\n",
    "- Twitter @kimberlyfessel \n",
    "- LinkedIn kimberlyfessel\n",
    "<br> \n",
    "\n",
    "<h2> <TT> Scraping Pipeline with Wikipedia </TT> </h2>\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9QWMD5T2fTw6"
   },
   "source": [
    "# Getting Started\n",
    "\n",
    "We will now be parsing information collected from the Internet, specifically from Wikipedia.  \n",
    "\n",
    "First let's take a look at the HTML source code that powers the page about Pennsylvania:\n",
    "- Open up https://en.wikipedia.org/wiki/Pennsylvania in your browser\n",
    "- Right click and select \"Inspect\" or \"Inspect Element\"\n",
    "- Alternatively:\n",
    "  - _Chrome_ -- View > Developer > View Source\n",
    "  - _Safari_ -- Develop > Show Web Inspector \n",
    "  - _Firefox_ -- Tools > Web Developer > Inspector\n",
    "\n",
    "The same HTML code we have been exploring is used to produce the structure of just about every webpage you visit.\n",
    "<br><br>\n",
    "\n",
    "Note: we will be scraping Wikipedia for learning purposes, but you can simply download its content instead.  Check [this](https://en.wikipedia.org/wiki/Wikipedia:Database_download) out to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yjx8WMbzfrXN"
   },
   "source": [
    "### Introduction to `requests`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n9Xs1aQNco7X"
   },
   "source": [
    "The Python library `requests` allows us to retrieve information from the web.  \n",
    "\n",
    "First we need to import this package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c0pKiQQode7l"
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EZbCLXa3dfE0"
   },
   "source": [
    "Now use the `.get()` method to retrieve a page's HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0wVJcLTYfF72"
   },
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Pennsylvania'\n",
    "\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SjZ7O93VdtsF"
   },
   "source": [
    "So, what's in a response?\n",
    "\n",
    "This object gives us a few important things:\n",
    "- `response.text` -- the returned HTML (if any)\n",
    "- `response.status_code` -- a [code](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) to tell you if your request was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nz9wWNxyepNs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code    #200 = success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bxragg2Ze1lo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n",
      "<head>\n",
      "<meta charset=\"UTF-8\"/>\n",
      "<title>Pennsylvania - Wikipedia</title>\n",
      "<script>document.documentElement.className=\"client-js\";RLCONF={\"w\n"
     ]
    }
   ],
   "source": [
    "print(response.text[:200])   #First 200 characters of the HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QL9ITc_QfL7S"
   },
   "outputs": [],
   "source": [
    "page = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ucRSFD5HcS2i"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P4eBpKvHf1-Y"
   },
   "source": [
    "### Using `requests` with `BeautifulSoup`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X1QgQNYj5n6r"
   },
   "source": [
    "Now that we have the HTML, we use `BeautifulSoup` to understand its structure in the exact same way as we did with sample HTML.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1OVCQBlNyA8RTkilbsMfEHP_7j000G1DP\"  alt=\"info\" height=\"100\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9GBwRv8vf5Fz"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mpZDj3Fr64iM"
   },
   "outputs": [],
   "source": [
    "soup = bs(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MP1LOpbBDUW6"
   },
   "source": [
    "`BeautifulSoup` has now parsed through the HTML about Pennsylvania, so we can look for things like the header tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UrGeeocu7BrC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1 class=\"firstHeading\" id=\"firstHeading\" lang=\"en\">Pennsylvania</h1>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wvYKC6Yh7BuO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pennsylvania'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('h1').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-0pckyOV_pOE"
   },
   "source": [
    "To extract information from the web, you will alternate between: \n",
    "- Inspecting the HTML in your browser\n",
    "- Using `BeautifulSoup` to find information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vPjwyfJj_qZ3"
   },
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sQIXQGJmDiZE"
   },
   "source": [
    "**Class Exercise - Disambiguation link**  <br> <br>\n",
    "Wikipedia often provides a disambiguation link to a list of additional topics that could reference the same term.  For example, searching for \"Pennsylvania\" directs to this article about the state, but \"Pennsylvania\" may instead refer to the railroad, a ship, or a music album.\n",
    "\n",
    ">Let's try to extract this disambiguation link by inspecting the source code and then using `BeautifulSoup`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rvbkg7X-Dhi7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"mw-disambig\" href=\"/wiki/Pennsylvania_(disambiguation)\" title=\"Pennsylvania (disambiguation)\">Pennsylvania (disambiguation)</a>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(class_='mw-disambig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tOwrqlKLDhmM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/wiki/Pennsylvania_(disambiguation)'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(class_='mw-disambig')['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_nYht_I1T7q4"
   },
   "source": [
    "**Exercise 1 - Longitute, Latitude**  <br>\n",
    ">How would you retrieve Pennsylvania's longitude and latitude coordinates from this page?\n",
    "\n",
    "<center>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1N43WC3jwFpV80L7-iLnEZi8aCQZtWcd3\"  alt=\"coordinates\" width=\"600\"/>\n",
    "</center>\n",
    "\n",
    "_Hint: Right click the coordinates directly and then select inspect._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G3DBinrGV4qC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'41°N 77.5°W'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(class_ = 'geo-dec').text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'41°N 77.5°W'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('span', class_ = 'geo-dec').text    # Being more specific and using two filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4mpgfcBD_vhj"
   },
   "source": [
    "# Advancing Further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3XwoIrTzgNzX"
   },
   "source": [
    "### Chaining commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "quW_Jg6oBwfg"
   },
   "source": [
    "In the last notebook, we learned that we could first isolate a division of the HTML and then look for tags within the division.  \n",
    "\n",
    "The returned element(s) of any `find()` or `find_all()` command are themselves `BeautifulSoup` elements.  This means we can continue searching for information within them.\n",
    "\n",
    "Let's take a look at the first table on the page.\n",
    "\n",
    "_Note: Adding `.prettify()` below just prints each HTML element on its own line._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iB3As00rf9x9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table class=\"infobox geography vcard\" style=\"width:22em;width:23em\">\n",
      " <tbody>\n",
      "  <tr>\n",
      "   <th colspan=\"2\" style=\"text-align:center;font-size:125%;font-weight:bold;font-size:1.25em; white-space:nowrap\">\n",
      "    <div class=\"fn org\" style=\"display:inline\">\n",
      "     Pennsylvania\n",
      "    </div>\n",
      "   </th>\n",
      "  </tr>\n",
      "  <tr>\n",
      "   <td colspan=\"2\" style=\"text-align:center;background-color:#cddeff; font-weight:bold;\">\n",
      "    <div class=\"category\">\n",
      "     <a href=\"/wiki/U.S._state\" title=\"U.S. state\">\n",
      "      State\n",
      "     </a>\n",
      "    </div>\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedtoprow\">\n",
      "   <td colspan=\"2\" style=\"text-align:center;font-weight:bold;\">\n",
      "    Commonwealth of Pennsylvania\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedtoprow\">\n",
      "   <td class=\"maptable\" colspan=\"2\" style=\"text-align:center\">\n",
      "    <div style=\"display:table; width:100%; background:none;\">\n",
      "     <div style=\"display:table-row\">\n",
      "      <div style=\"display:table-cell;vertical-align:middle; text-align:center;\">\n",
      "       <a class=\"image\" href=\"/wiki/File:Flag_of_Pennsylvania.svg\" title=\"Flag of Pennsylvania\">\n",
      "        <img alt=\"Flag of Pennsylvania\" class=\"thumbborder\" data-file-height=\"450\" data-file-width=\"675\" decoding=\"async\" height=\"83\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/f/f7/Flag_of_Pennsylvania.svg/125px-Flag_of_Pennsylvania.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/f/f7/Flag_of_Pennsylvania.svg/188px-Flag_of_Pennsylvania.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/f7/Flag_of_Pennsylvania.svg/250px-Flag_of_Pennsylvania.svg.png 2x\" width=\"125\"/>\n",
      "       </a>\n",
      "       <div style=\"padding:0.2em 0 0.2em 0;\">\n",
      "        <a class=\"mw-redirect\" href=\"/wiki/Flag_of_Pennsylvania\" title=\"Flag of Pennsylvania\">\n",
      "         Flag\n",
      "        </a>\n",
      "       </div>\n",
      "      </div>\n",
      "      <div style=\"display:table-cell;vertical-align:middle; text-align:center;\">\n",
      "       <a class=\"image\" href=\"/wiki/File:Seal_of_Pennsylvania.svg\" title=\"Official seal of Pennsylvania\">\n",
      "        <img alt=\"Official seal of Pennsylvania\" data-file-height=\"289\" data-file-width=\"289\" decoding=\"async\" height=\"100\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/7/7d/Seal_of_Pennsylvania.svg/100px-Seal_of_Pennsylvania.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/7/7d/Seal_of_Pennsylvania.svg/150px-Seal_of_Pennsylvania.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/7/7d/Seal_of_Pennsylvania.svg/200px-Seal_of_Pennsylvania.svg.png 2x\" width=\"100\"/>\n",
      "       </a>\n",
      "       <div style=\"padding:0.2em 0 0.2em 0;\">\n",
      "        <a href=\"/wiki/Seal_of_Pennsylvania\" title=\"Seal of Pennsylvania\">\n",
      "         Seal\n",
      "        </a>\n",
      "       </div>\n",
      "      </div>\n",
      "     </div>\n",
      "    </div>\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedtoprow\">\n",
      "   <td colspan=\"2\" style=\"text-align:center\">\n",
      "    <a class=\"mw-redirect\" href=\"/wiki/List_of_U.S._state_nicknames\" title=\"List of U.S. state nicknames\">\n",
      "     Nickname(s):\n",
      "    </a>\n",
      "    <div class=\"nickname\" style=\"display:inline\">\n",
      "     Keystone State;\n",
      "     <sup class=\"reference\" id=\"cite_ref-1\">\n",
      "      <a href=\"#cite_note-1\">\n",
      "       [1]\n",
      "      </a>\n",
      "     </sup>\n",
      "     Quaker State\n",
      "    </div>\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <td colspan=\"2\" style=\"text-align:center\">\n",
      "    <a href=\"/wiki/List_of_U.S._state_and_territory_mottos\" title=\"List of U.S. state and territory mottos\">\n",
      "     Motto(s):\n",
      "    </a>\n",
      "    <div class=\"nickname\" style=\"display:inline\">\n",
      "     Virtue, Liberty and Independence\n",
      "    </div>\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <td colspan=\"2\" style=\"text-align:center\">\n",
      "    <a href=\"/wiki/List_of_U.S._state_songs\" title=\"List of U.S. state songs\">\n",
      "     Anthem:\n",
      "    </a>\n",
      "    \"\n",
      "    <a href=\"/wiki/Pennsylvania_(song)\" title=\"Pennsylvania (song)\">\n",
      "     Pennsylvania\n",
      "    </a>\n",
      "    \"\n",
      "    <br/>\n",
      "    <div class=\"center\">\n",
      "     <div class=\"floatnone\">\n",
      "      <div class=\"mediaContainer\" style=\"width:220px\">\n",
      "       <audio class=\"kskin\" controls=\"\" data-durationhint=\"75.359614512472\" data-mwprovider=\"wikimediacommons\" data-mwtitle='\"Pennsylvania\"_-_Regional_anthem_of_Pennsylvania.ogg' data-startoffset=\"0\" id=\"mwe_player_0\" preload=\"none\" style=\"width:220px\">\n",
      "        <source data-bandwidth=\"210704\" data-height=\"0\" data-shorttitle=\"MP3\" data-title=\"MP3\" data-transcodekey=\"mp3\" data-width=\"0\" src=\"//upload.wikimedia.org/wikipedia/commons/transcoded/9/9c/%22Pennsylvania%22_-_Regional_anthem_of_Pennsylvania.ogg/%22Pennsylvania%22_-_Regional_anthem_of_Pennsylvania.ogg.mp3\" type=\"audio/mpeg\"/>\n",
      "        <source data-bandwidth=\"348997\" data-height=\"0\" data-shorttitle=\"Ogg source\" data-title=\"Original Ogg file (349 kbps)\" data-width=\"0\" src=\"//upload.wikimedia.org/wikipedia/commons/9/9c/%22Pennsylvania%22_-_Regional_anthem_of_Pennsylvania.ogg\" type='audio/ogg; codecs=\"vorbis\"'/>\n",
      "       </audio>\n",
      "      </div>\n",
      "     </div>\n",
      "    </div>\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedtoprow\">\n",
      "   <td colspan=\"2\" style=\"text-align:center\">\n",
      "    <a class=\"image\" href=\"/wiki/File:Pennsylvania_in_United_States.svg\" title=\"Map of the United States with Pennsylvania highlighted\">\n",
      "     <img alt=\"Map of the United States with Pennsylvania highlighted\" data-file-height=\"731\" data-file-width=\"1181\" decoding=\"async\" height=\"186\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Pennsylvania_in_United_States.svg/300px-Pennsylvania_in_United_States.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Pennsylvania_in_United_States.svg/450px-Pennsylvania_in_United_States.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Pennsylvania_in_United_States.svg/600px-Pennsylvania_in_United_States.svg.png 2x\" width=\"300\"/>\n",
      "    </a>\n",
      "    <div style=\"padding:0.3em 0 0 0;\">\n",
      "     Map of the United States with Pennsylvania highlighted\n",
      "    </div>\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedtoprow\">\n",
      "   <th scope=\"row\">\n",
      "    Country\n",
      "   </th>\n",
      "   <td>\n",
      "    <a href=\"/wiki/United_States\" title=\"United States\">\n",
      "     United States\n",
      "    </a>\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    Before statehood\n",
      "   </th>\n",
      "   <td>\n",
      "    <a href=\"/wiki/Province_of_Pennsylvania\" title=\"Province of Pennsylvania\">\n",
      "     Province of Pennsylvania\n",
      "    </a>\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    <a href=\"/wiki/List_of_U.S._states_by_date_of_admission_to_the_Union#List_of_U.S._states\" title=\"List of U.S. states by date of admission to the Union\">\n",
      "     Admitted to the Union\n",
      "    </a>\n",
      "   </th>\n",
      "   <td>\n",
      "    December 12, 1787 (2nd)\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedtoprow\">\n",
      "   <th scope=\"row\">\n",
      "    <a href=\"/wiki/List_of_capitals_in_the_United_States\" title=\"List of capitals in the United States\">\n",
      "     Capital\n",
      "    </a>\n",
      "   </th>\n",
      "   <td>\n",
      "    <a href=\"/wiki/Harrisburg,_Pennsylvania\" title=\"Harrisburg, Pennsylvania\">\n",
      "     Harrisburg\n",
      "    </a>\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    <a class=\"mw-redirect\" href=\"/wiki/List_of_U.S._states%27_largest_cities_by_population\" title=\"List of U.S. states' largest cities by population\">\n",
      "     Largest city\n",
      "    </a>\n",
      "   </th>\n",
      "   <td>\n",
      "    <a href=\"/wiki/Philadelphia\" title=\"Philadelphia\">\n",
      "     Philadelphia\n",
      "    </a>\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    <a class=\"mw-redirect\" href=\"/wiki/List_of_Metropolitan_Statistical_Areas\" title=\"List of Metropolitan Statistical Areas\">\n",
      "     Largest metro\n",
      "    </a>\n",
      "   </th>\n",
      "   <td>\n",
      "    <a href=\"/wiki/Delaware_Valley\" title=\"Delaware Valley\">\n",
      "     Delaware Valley\n",
      "    </a>\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedtoprow\">\n",
      "   <th colspan=\"2\" style=\"text-align:center;text-align:left\">\n",
      "    Government\n",
      "    <div style=\"font-weight:normal;display:inline;\">\n",
      "    </div>\n",
      "   </th>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    •\n",
      "    <a class=\"mw-redirect\" href=\"/wiki/Governor_of_Pennsylvania\" title=\"Governor of Pennsylvania\">\n",
      "     Governor\n",
      "    </a>\n",
      "   </th>\n",
      "   <td>\n",
      "    <span class=\"nowrap\">\n",
      "     <a href=\"/wiki/Tom_Wolf\" title=\"Tom Wolf\">\n",
      "      Tom Wolf\n",
      "     </a>\n",
      "     (\n",
      "     <a href=\"/wiki/Democratic_Party_(United_States)\" title=\"Democratic Party (United States)\">\n",
      "      D\n",
      "     </a>\n",
      "     )\n",
      "    </span>\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    •\n",
      "    <a href=\"/wiki/Lieutenant_Governor_of_Pennsylvania\" title=\"Lieutenant Governor of Pennsylvania\">\n",
      "     Lieutenant Governor\n",
      "    </a>\n",
      "   </th>\n",
      "   <td>\n",
      "    <span class=\"nowrap\">\n",
      "     <a href=\"/wiki/John_Fetterman_(politician)\" title=\"John Fetterman (politician)\">\n",
      "      John Fetterman\n",
      "     </a>\n",
      "     (D)\n",
      "    </span>\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    <a href=\"/wiki/Legislature\" title=\"Legislature\">\n",
      "     Legislature\n",
      "    </a>\n",
      "   </th>\n",
      "   <td>\n",
      "    <span class=\"nowrap\">\n",
      "     <a href=\"/wiki/Pennsylvania_General_Assembly\" title=\"Pennsylvania General Assembly\">\n",
      "      General Assembly\n",
      "     </a>\n",
      "    </span>\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    •\n",
      "    <a href=\"/wiki/Upper_house\" title=\"Upper house\">\n",
      "     Upper house\n",
      "    </a>\n",
      "   </th>\n",
      "   <td>\n",
      "    <a href=\"/wiki/Pennsylvania_State_Senate\" title=\"Pennsylvania State Senate\">\n",
      "     State Senate\n",
      "    </a>\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    •\n",
      "    <a href=\"/wiki/Lower_house\" title=\"Lower house\">\n",
      "     Lower house\n",
      "    </a>\n",
      "   </th>\n",
      "   <td>\n",
      "    <a href=\"/wiki/Pennsylvania_House_of_Representatives\" title=\"Pennsylvania House of Representatives\">\n",
      "     House of Representatives\n",
      "    </a>\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    <a href=\"/wiki/List_of_United_States_senators_from_Pennsylvania\" title=\"List of United States senators from Pennsylvania\">\n",
      "     U.S. senators\n",
      "    </a>\n",
      "   </th>\n",
      "   <td>\n",
      "    <span class=\"nowrap\">\n",
      "     <a href=\"/wiki/Bob_Casey_Jr.\" title=\"Bob Casey Jr.\">\n",
      "      Bob Casey Jr.\n",
      "     </a>\n",
      "     (D)\n",
      "    </span>\n",
      "    <br/>\n",
      "    <span class=\"nowrap\">\n",
      "     <a href=\"/wiki/Pat_Toomey\" title=\"Pat Toomey\">\n",
      "      Pat Toomey\n",
      "     </a>\n",
      "     (\n",
      "     <a href=\"/wiki/Republican_Party_(United_States)\" title=\"Republican Party (United States)\">\n",
      "      R\n",
      "     </a>\n",
      "     )\n",
      "    </span>\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    <a href=\"/wiki/United_States_House_of_Representatives\" title=\"United States House of Representatives\">\n",
      "     U.S. House delegation\n",
      "    </a>\n",
      "   </th>\n",
      "   <td>\n",
      "    9 Democrats\n",
      "    <br/>\n",
      "    9 Republicans (\n",
      "    <a href=\"/wiki/United_States_congressional_delegations_from_Pennsylvania\" title=\"United States congressional delegations from Pennsylvania\">\n",
      "     list\n",
      "    </a>\n",
      "    )\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedtoprow\">\n",
      "   <th colspan=\"2\" style=\"text-align:center;text-align:left\">\n",
      "    Area\n",
      "    <div style=\"font-weight:normal;display:inline;\">\n",
      "    </div>\n",
      "   </th>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    • Total\n",
      "   </th>\n",
      "   <td>\n",
      "    46,055 sq mi (119,283 km\n",
      "    <sup>\n",
      "     2\n",
      "    </sup>\n",
      "    )\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    • Land\n",
      "   </th>\n",
      "   <td>\n",
      "    44,816.61 sq mi (116,074 km\n",
      "    <sup>\n",
      "     2\n",
      "    </sup>\n",
      "    )\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    • Water\n",
      "   </th>\n",
      "   <td>\n",
      "    1,239 sq mi (3,208 km\n",
      "    <sup>\n",
      "     2\n",
      "    </sup>\n",
      "    )  2.7%\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    Area rank\n",
      "   </th>\n",
      "   <td>\n",
      "    <a href=\"/wiki/List_of_U.S._states_and_territories_by_area\" title=\"List of U.S. states and territories by area\">\n",
      "     33rd\n",
      "    </a>\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedtoprow\">\n",
      "   <th colspan=\"2\" style=\"text-align:center;text-align:left\">\n",
      "    Dimensions\n",
      "    <div style=\"font-weight:normal;display:inline;\">\n",
      "    </div>\n",
      "   </th>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    • Length\n",
      "   </th>\n",
      "   <td>\n",
      "    170 mi (273 km)\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    • Width\n",
      "   </th>\n",
      "   <td>\n",
      "    283 mi (455 km)\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedtoprow\">\n",
      "   <th scope=\"row\">\n",
      "    Elevation\n",
      "    <div style=\"font-weight:normal;display:inline;\">\n",
      "    </div>\n",
      "   </th>\n",
      "   <td>\n",
      "    1,100 ft (340 m)\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    Highest elevation\n",
      "    <div style=\"font-weight:normal;display:inline;\">\n",
      "     (\n",
      "     <a href=\"/wiki/Mount_Davis_(Pennsylvania)\" title=\"Mount Davis (Pennsylvania)\">\n",
      "      Mount Davis\n",
      "     </a>\n",
      "     <sup class=\"reference\" id=\"cite_ref-USGS_2-0\">\n",
      "      <a href=\"#cite_note-USGS-2\">\n",
      "       [2]\n",
      "      </a>\n",
      "     </sup>\n",
      "     <sup class=\"reference\" id=\"cite_ref-AVD88_3-0\">\n",
      "      <a href=\"#cite_note-AVD88-3\">\n",
      "       [3]\n",
      "      </a>\n",
      "     </sup>\n",
      "     )\n",
      "    </div>\n",
      "   </th>\n",
      "   <td>\n",
      "    3,213 ft (979 m)\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedbottomrow\">\n",
      "   <th scope=\"row\">\n",
      "    Lowest elevation\n",
      "    <div style=\"font-weight:normal;display:inline;\">\n",
      "     (\n",
      "     <a href=\"/wiki/Delaware_River\" title=\"Delaware River\">\n",
      "      Delaware River\n",
      "     </a>\n",
      "     at\n",
      "     <span class=\"nowrap\">\n",
      "      <a href=\"/wiki/Delaware\" title=\"Delaware\">\n",
      "       Delaware\n",
      "      </a>\n",
      "      border\n",
      "     </span>\n",
      "     <sup class=\"reference\" id=\"cite_ref-USGS_2-1\">\n",
      "      <a href=\"#cite_note-USGS-2\">\n",
      "       [2]\n",
      "      </a>\n",
      "     </sup>\n",
      "     )\n",
      "    </div>\n",
      "   </th>\n",
      "   <td>\n",
      "    0 ft (0 m)\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedtoprow\">\n",
      "   <th colspan=\"2\" style=\"text-align:center;text-align:left\">\n",
      "    Population\n",
      "    <div style=\"font-weight:normal;display:inline;\">\n",
      "     <span class=\"nowrap\">\n",
      "     </span>\n",
      "     (2019)\n",
      "    </div>\n",
      "   </th>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    • Total\n",
      "   </th>\n",
      "   <td>\n",
      "    12,801,989\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    • Rank\n",
      "   </th>\n",
      "   <td>\n",
      "    <a href=\"/wiki/List_of_states_and_territories_of_the_United_States_by_population\" title=\"List of states and territories of the United States by population\">\n",
      "     5th\n",
      "    </a>\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    • Density\n",
      "   </th>\n",
      "   <td>\n",
      "    284/sq mi (110/km\n",
      "    <sup>\n",
      "     2\n",
      "    </sup>\n",
      "    )\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    • Density rank\n",
      "   </th>\n",
      "   <td>\n",
      "    <a href=\"/wiki/List_of_states_and_territories_of_the_United_States_by_population_density\" title=\"List of states and territories of the United States by population density\">\n",
      "     9th\n",
      "    </a>\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    •\n",
      "    <a href=\"/wiki/Household_income_in_the_United_States#Income_by_state\" title=\"Household income in the United States\">\n",
      "     Median household income\n",
      "    </a>\n",
      "    <div style=\"font-weight:normal;display:inline;\">\n",
      "    </div>\n",
      "   </th>\n",
      "   <td>\n",
      "    $59,195\n",
      "    <sup class=\"reference\" id=\"cite_ref-4\">\n",
      "     <a href=\"#cite_note-4\">\n",
      "      [4]\n",
      "     </a>\n",
      "    </sup>\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    • Income rank\n",
      "    <div style=\"font-weight:normal;display:inline;\">\n",
      "    </div>\n",
      "   </th>\n",
      "   <td>\n",
      "    25th\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedtoprow\">\n",
      "   <th scope=\"row\">\n",
      "    <a href=\"/wiki/Demonym\" title=\"Demonym\">\n",
      "     Demonym(s)\n",
      "    </a>\n",
      "   </th>\n",
      "   <td>\n",
      "    Pennsylvanian\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedtoprow\">\n",
      "   <th colspan=\"2\" style=\"text-align:center;text-align:left\">\n",
      "    Language\n",
      "    <div style=\"font-weight:normal;display:inline;\">\n",
      "    </div>\n",
      "   </th>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    •\n",
      "    <a href=\"/wiki/Languages_of_the_United_States\" title=\"Languages of the United States\">\n",
      "     Official language\n",
      "    </a>\n",
      "   </th>\n",
      "   <td>\n",
      "    None\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    •\n",
      "    <a href=\"/wiki/Languages_of_the_United_States\" title=\"Languages of the United States\">\n",
      "     Spoken language\n",
      "    </a>\n",
      "   </th>\n",
      "   <td>\n",
      "    <a href=\"/wiki/English_language\" title=\"English language\">\n",
      "     English\n",
      "    </a>\n",
      "    90.15%\n",
      "    <br/>\n",
      "    <a href=\"/wiki/Spanish_language\" title=\"Spanish language\">\n",
      "     Spanish\n",
      "    </a>\n",
      "    4.09%\n",
      "    <br/>\n",
      "    <a href=\"/wiki/German_language\" title=\"German language\">\n",
      "     German\n",
      "    </a>\n",
      "    (Including\n",
      "    <a href=\"/wiki/Pennsylvania_German_language\" title=\"Pennsylvania German language\">\n",
      "     Pennsylvania German\n",
      "    </a>\n",
      "    ) 0.87%\n",
      "    <br/>\n",
      "    <a href=\"/wiki/Chinese_language\" title=\"Chinese language\">\n",
      "     Chinese\n",
      "    </a>\n",
      "    0.47%\n",
      "    <br/>\n",
      "    <a href=\"/wiki/Italian_language\" title=\"Italian language\">\n",
      "     Italian\n",
      "    </a>\n",
      "    0.43%\n",
      "    <sup class=\"reference\" id=\"cite_ref-5\">\n",
      "     <a href=\"#cite_note-5\">\n",
      "      [5]\n",
      "     </a>\n",
      "    </sup>\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedtoprow\">\n",
      "   <th scope=\"row\">\n",
      "    <a href=\"/wiki/Time_zone\" title=\"Time zone\">\n",
      "     Time zone\n",
      "    </a>\n",
      "   </th>\n",
      "   <td>\n",
      "    <a href=\"/wiki/UTC%E2%88%9205:00\" title=\"UTC−05:00\">\n",
      "     UTC−05:00\n",
      "    </a>\n",
      "    (\n",
      "    <a href=\"/wiki/Eastern_Time_Zone\" title=\"Eastern Time Zone\">\n",
      "     Eastern\n",
      "    </a>\n",
      "    )\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    <span style=\"white-space:nowrap\">\n",
      "     • Summer (\n",
      "     <a href=\"/wiki/Daylight_saving_time\" title=\"Daylight saving time\">\n",
      "      DST\n",
      "     </a>\n",
      "     )\n",
      "    </span>\n",
      "   </th>\n",
      "   <td>\n",
      "    <a href=\"/wiki/UTC%E2%88%9204:00\" title=\"UTC−04:00\">\n",
      "     UTC−04:00\n",
      "    </a>\n",
      "    (\n",
      "    <a class=\"mw-redirect\" href=\"/wiki/Eastern_Daylight_Time\" title=\"Eastern Daylight Time\">\n",
      "     EDT\n",
      "    </a>\n",
      "    )\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedtoprow\">\n",
      "   <th scope=\"row\">\n",
      "    <a href=\"/wiki/List_of_U.S._state_abbreviations#Postal_codes\" title=\"List of U.S. state abbreviations\">\n",
      "     USPS abbreviation\n",
      "    </a>\n",
      "   </th>\n",
      "   <td class=\"adr\">\n",
      "    <div class=\"postal-code\">\n",
      "     PA\n",
      "    </div>\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    <a href=\"/wiki/ISO_3166\" title=\"ISO 3166\">\n",
      "     ISO 3166 code\n",
      "    </a>\n",
      "   </th>\n",
      "   <td class=\"nickname\">\n",
      "    <a href=\"/wiki/ISO_3166-2:US\" title=\"ISO 3166-2:US\">\n",
      "     US-PA\n",
      "    </a>\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr>\n",
      "   <th scope=\"row\">\n",
      "    <a href=\"/wiki/List_of_U.S._state_abbreviations#Current_use_of_traditional_abbreviations\" title=\"List of U.S. state abbreviations\">\n",
      "     Trad. abbreviation\n",
      "    </a>\n",
      "   </th>\n",
      "   <td class=\"nickname\">\n",
      "    Pa., Penn., Penna.\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedtoprow\">\n",
      "   <th scope=\"row\">\n",
      "    Latitude\n",
      "   </th>\n",
      "   <td>\n",
      "    39°43′ to 42°16′ N\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedrow\">\n",
      "   <th scope=\"row\">\n",
      "    Longitude\n",
      "   </th>\n",
      "   <td>\n",
      "    74°41′ to 80°31′ W\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr class=\"mergedtoprow\">\n",
      "   <th scope=\"row\">\n",
      "    Website\n",
      "   </th>\n",
      "   <td>\n",
      "    <span class=\"url\">\n",
      "     <a class=\"external text\" href=\"https://www.pa.gov/\" rel=\"nofollow\">\n",
      "      www\n",
      "      <wbr/>\n",
      "      .pa\n",
      "      <wbr/>\n",
      "      .gov\n",
      "     </a>\n",
      "    </span>\n",
      "   </td>\n",
      "  </tr>\n",
      " </tbody>\n",
      "</table>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.find('table').prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XXz_zkwDPBze"
   },
   "outputs": [],
   "source": [
    "first_table = soup.find('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NH8Q6MxzPEdD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(first_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IMLAM5oXPNWC"
   },
   "source": [
    "Now find the header text of this table and the text of the first data row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XFxzeQVoPGpA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pennsylvania'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_table.find('th').text  # 'th' -> table header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YjaSVBb3PooR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'State'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_table.find('td').text  # 'td' -> table data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J4bY3_ZdP7fN"
   },
   "source": [
    "Also note that instead of saving the table as its own Python variable, you could just chain these searches together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jNvIXdLrQDx3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pennsylvania'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = soup.find('table').find('th').text\n",
    "\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-usqIVOkQH2g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pennsylvania\n",
      "State\n",
      "Commonwealth of Pennsylvania\n",
      "\n",
      "FlagSeal\n",
      "Nickname(s): Keystone State;[1] Quaker State\n",
      "Motto(s): Virtue, Liberty and Independence\n",
      "Anthem: \"Pennsylvania\"\n",
      "Map of the United States with Pennsylvania highlighted\n",
      "CountryUnited States\n",
      "Before statehoodProvince of Pennsylvania\n"
     ]
    }
   ],
   "source": [
    "for row in soup.find('table').find_all('tr')[:10]:  # Iterate over all the table row -> 'tr' and only represent the 1st 11 rows\n",
    "  print(row.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mE2v84bMQkCX"
   },
   "source": [
    "You can continue chaining down through as much of the HTML DOM as you'd like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "THrfXwcVQn1C"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This article is about the U.S. state. For other uses, see Pennsylvania (disambiguation).'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(soup\n",
    " .find('div', id='content')\n",
    " .find('div', id='bodyContent')\n",
    " .find('div', id='mw-content-text')\n",
    " .find('div', role='note')\n",
    ").text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zodKBCMEgVuA"
   },
   "source": [
    "### Locating information by position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hfOiDJnoR38I"
   },
   "source": [
    "We just saw that basic facts about Pennsylvania can be found within the first table of this page.  Now let's get more specific.\n",
    "\n",
    "How can we extract the date Pennsylvania was admitted to the union?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lAxIUSlQf91o"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<td>December 12, 1787 (2nd)</td>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('table').find_all('td')[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tWbjVOiHW7e0"
   },
   "source": [
    "It's the tenth element in this list, but what happens to this code if someone edits the Wikipedia table to include additional information?\n",
    "\n",
    "\n",
    "Sometimes it is better or necessary to find information by text matching, but be careful -- this needs to be an exact match!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8-2VQAnRWoaF"
   },
   "outputs": [],
   "source": [
    "soup.find(text='Admitted')   #not an exact match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OTegKoctWofN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Admitted to the Union'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(text='Admitted to the Union')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SS0qEJE3YLfr"
   },
   "source": [
    "Alternatively, we could use [regular expressions](https://docs.python.org/3/library/re.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oQ13X3m3YSX-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Admitted to the Union'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "admitted_regex = re.compile('Admitted')\n",
    "soup.find(text=admitted_regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NKfTw-_gX59C"
   },
   "source": [
    "This looks like a string, but it's actually a `BeautifulSoup` element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4alYF2KjX08B"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.NavigableString"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admitted = soup.find(text='Admitted to the Union')\n",
    "\n",
    "type(admitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZmrhV9LlYlpG"
   },
   "source": [
    "So we can use it to traverse the DOM.  Here, we will find the next element in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uwSpaWfHYAe3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<td>December 12, 1787 (2nd)</td>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admitted.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "80qNTf8ZYvmW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'December 12, 1787 (2nd)'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admitted.next.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EQkvt_m0Y0nN"
   },
   "source": [
    "For some cases it's much easier to find one element and then move up, down, or sideways within the DOM.  `BeautifulSoup` also allows you to look for `.parent`, `.children`, `.next_sibling`, `.previous_sibling`, [etc.](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#navigating-the-tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s66gL4QpckiW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"/wiki/List_of_U.S._states_by_date_of_admission_to_the_Union#List_of_U.S._states\" title=\"List of U.S. states by date of admission to the Union\">Admitted to the Union</a>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admitted.parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sj3q_tlTc6bi"
   },
   "source": [
    "**Tip**: Any \"plural\" attribute such as `children` or `siblings` will return a generator.  Just loop over the result or convert it to a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BF542FNlgcfB"
   },
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HlHPqxXCdFYn"
   },
   "source": [
    "**Exercise 2 - Capital City**  <br>\n",
    ">Write code to extract the capital of Pennsylvania from the main table without using list positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tEeBmiEIf-C3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Harrisburg'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capital_regex = re.compile(\"Capital\")\n",
    "capital = soup.find(text=capital_regex)\n",
    "capital.next.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hRRffF8Ed9DU"
   },
   "source": [
    "**Exercise 3 - Reference Links**  <br>\n",
    "> Print out the text of the first three references (at the bottom of the page).  For an added bonus: can you also print all the external links from these three references?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sxv-Z-nff2ke"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Symbols of Pennsylvania\". Portal.state.pa.us. Archived from the original on October 14, 2007. Retrieved May 4, 2014.\n",
      "\"Elevations and Distances in the United States\". United States Geological Survey. 2001. Archived from the original on October 15, 2011. Retrieved October 24, 2011.\n",
      "\"Median Annual Household Income\". The Henry J. Kaiser Family Foundation. Archived from the original on December 20, 2016. Retrieved December 9, 2016.\n"
     ]
    }
   ],
   "source": [
    "ref3 = soup.find(class_ = 'references').find_all('cite')[:3]  # Look for the references section and get the 3 citations\n",
    "for ref in ref3:\n",
    "    print(ref.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sSFnIqnIf2oN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.portal.state.pa.us/portal/server.pt/community/things/4280/symbols_of_pennsylvania/478690\n",
      "https://web.archive.org/web/20071014215922/http://www.phmc.state.pa.us/bah/pahist/symbols.asp?secid=31\n",
      "https://web.archive.org/web/20111015012701/http://egsc.usgs.gov/isb/pubs/booklets/elvadist/elvadist.html\n",
      "/wiki/United_States_Geological_Survey\n",
      "http://egsc.usgs.gov/isb/pubs/booklets/elvadist/elvadist.html\n",
      "http://kff.org/other/state-indicator/median-annual-income/?currentTimeframe=0\n",
      "https://web.archive.org/web/20161220091007/http://kff.org/other/state-indicator/median-annual-income/?currentTimeframe=0\n"
     ]
    }
   ],
   "source": [
    "ref3 = soup.find(class_ = 'references').find_all('cite')[:3]\n",
    "\n",
    "# Need to get the link on the 3 first citations\n",
    "for ref in ref3:\n",
    "    for link in ref.find_all('a'):\n",
    "        print(link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.portal.state.pa.us/portal/server.pt/community/things/4280/symbols_of_pennsylvania/478690\n",
      "https://web.archive.org/web/20071014215922/http://www.phmc.state.pa.us/bah/pahist/symbols.asp?secid=31\n",
      "https://web.archive.org/web/20111015012701/http://egsc.usgs.gov/isb/pubs/booklets/elvadist/elvadist.html\n",
      "http://egsc.usgs.gov/isb/pubs/booklets/elvadist/elvadist.html\n",
      "http://kff.org/other/state-indicator/median-annual-income/?currentTimeframe=0\n",
      "https://web.archive.org/web/20161220091007/http://kff.org/other/state-indicator/median-annual-income/?currentTimeframe=0\n"
     ]
    }
   ],
   "source": [
    "# Only keep the external links\n",
    "ref3 = soup.find(class_ = 'references').find_all('cite')[:3]\n",
    "\n",
    "# Need to get the link on the 3 first citations\n",
    "for ref in ref3:\n",
    "    for link in ref.find_all('a', class_ = 'external'):\n",
    "        print(link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1YvSGjDegu3j"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "28UdiLXBJm8Z"
   },
   "source": [
    "Now that we know how to gather information from the web, what do we do with it?\n",
    "\n",
    "This data can be\n",
    "- aggregated to look for trends\n",
    "- visualized to understand patterns\n",
    "- leveraged with machine learning algorithms\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DkZVzA4zNJrK"
   },
   "source": [
    "\n",
    "But first we need to \n",
    "- convert several strings into numerical or datetime values\n",
    "- collect and store data from multiple pages (next section)\n",
    "\n",
    "**Tip**: Most web scraping project rely on multiple pages of information, each of which serving as a data observation.  For this case, we might collect data about Pennsylvania and then collect the same kinds of information for all 50 United States before analyzing or visualizing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eP-3auSQg-xN"
   },
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D9ud6P0Q-ZD-"
   },
   "source": [
    "#### Date Admitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K7qelh9YWsbk"
   },
   "source": [
    "In the last section, we collected the date that Pennsylvania was admitted to the union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "btM7W50vgfAu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'December 12, 1787 (2nd)'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_admitted_text = admitted.next.text\n",
    "\n",
    "date_admitted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7mBTAYL2W_eZ"
   },
   "source": [
    "We need Python to recognize this as a date for futher analyses.  First let's narrow down to just the date part of the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LEaAturdgfDq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['December', '12,', '1787']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_admitted_list = date_admitted_text.split(' ')[:-1]  # Create a list and drop last item\n",
    "\n",
    "date_admitted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H7476gCpXLaI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'December 12, 1787'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_admitted_str = ' '.join(date_admitted_list)   # Create a string from a list using .join()\n",
    "\n",
    "date_admitted_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iuJW2pd_9d4g"
   },
   "source": [
    "Now we will convert this string into a datetime data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "86QHOMcfXLeU"
   },
   "outputs": [],
   "source": [
    "import dateutil.parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cca3A6CPyMBF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(1787, 12, 12, 0, 0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_admitted = dateutil.parser.parse(date_admitted_str)\n",
    "\n",
    "date_admitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VxKYD4pE-pqv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(date_admitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "euH7jBO3yMEg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1787"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_admitted.year   # datetime has attribute .year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lY3WMJuD-tL8"
   },
   "source": [
    "#### Population and Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_uuoGoQs_Cb0"
   },
   "source": [
    "Another quantity that might be useful if we want to compare Pennsylvania to other US states is population.  Let's look for the word \"Total\" and use the same trick we tried before.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aFNJugfe-_rK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\xa0•\\xa0Total'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(text=re.compile('Total'))   # Important to use regex because the text includes the dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N3TkqAto_1YQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<td>46,055 sq mi (119,283 km<sup>2</sup>)</td>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(text=re.compile('Total')).next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dk07aajPAmYQ"
   },
   "source": [
    "That's not the population!  Looks like total area is also next to a \"Total\" label.  Let's save that and come back to it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uNF67hL1_1bK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'46,055\\xa0sq\\xa0mi (119,283\\xa0km2)'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area_text = soup.find(text=re.compile('Total')).next.text\n",
    "area_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oKZPzfF-A4l3"
   },
   "source": [
    "How might we explicitly look for the population total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VbEecQH8_1VZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<th colspan=\"2\" style=\"text-align:center;text-align:left\">Population<div style=\"font-weight:normal;display:inline;\"><span class=\"nowrap\"> </span>(2019)</div></th>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(text='Population').parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KObG3r-qB888"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tr class=\"mergedtoprow\"><th colspan=\"2\" style=\"text-align:center;text-align:left\">Population<div style=\"font-weight:normal;display:inline;\"><span class=\"nowrap\"> </span>(2019)</div></th></tr>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(text='Population').parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QwStF7qwCB1l"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tr class=\"mergedrow\"><th scope=\"row\"> • Total</th><td>12,801,989</td></tr>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(text='Population').parent.parent.next_sibling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wDbsBNhVCO90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<td>12,801,989</td>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(text='Population').parent.parent.next_sibling.find('td')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lkG11Dz7CRhK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12,801,989'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "population_text = soup.find(text='Population').parent.parent.next_sibling.find('td').text\n",
    "\n",
    "population_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yOCvBOy8CbSx"
   },
   "source": [
    "Sometimes you need to continuing traversing the DOM until you find the information you need!\n",
    "\n",
    "Now let's convert that string into an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g4jYh8ZHCai4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12801989"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "population = int(population_text.replace(',', ''))\n",
    "\n",
    "population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z0HqtiCHDBHK"
   },
   "source": [
    "Often it's useful to write functions to help you clean up your data.  Let's do that now so we can reuse these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n7jwlEHYCouv"
   },
   "outputs": [],
   "source": [
    "def to_date(date_str):\n",
    "    date_str = re.match('[\\w\\s,]+', date_str)[0]\n",
    "    return dateutil.parser.parse(date_str)\n",
    "\n",
    "def to_int(number_str):\n",
    "    number_str = re.match('[\\d,$]+', number_str)[0]\n",
    "    number_str = number_str.replace('$', '').replace(',', '')\n",
    "    return int(number_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iUVZ_XOWDfAh"
   },
   "source": [
    "Now we can use our `to_int` function to clean up the area text we found previously.  This text actually contains special spaces so we will use regular expressions (regex) to capture just the first digits in the `to_int` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xSIEej9sCoxh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'46,055\\xa0sq\\xa0mi (119,283\\xa0km2)'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fHgw0IbbDemn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46055"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area = to_int(area_text)\n",
    "\n",
    "area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ZmaFPGBhFaT"
   },
   "source": [
    "### Data storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8yLd9Qc6EdF6"
   },
   "source": [
    "Now let's put all the information we have about Pennsylvania together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t2BxYfFDgfGi"
   },
   "outputs": [],
   "source": [
    "penn_dict = {\n",
    "    'state': state,\n",
    "    'date_admitted': date_admitted,\n",
    "    'population': population,\n",
    "    'area_sq_mi': area\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "91QNYIWnEcjH",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 'Pennsylvania',\n",
       " 'date_admitted': datetime.datetime(1787, 12, 12, 0, 0),\n",
       " 'population': 12801989,\n",
       " 'area_sq_mi': 46055}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penn_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BXCpF_Q6KtS9"
   },
   "source": [
    "Once we have this information in dictionary form, we can build a `pandas` dataframe with it and eventually perform further analyses or save it to our computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZQS-jkQiEcmn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h9f47eTJK6H0"
   },
   "outputs": [],
   "source": [
    "penn_info = [penn_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AtT07O1XKnCb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>date_admitted</th>\n",
       "      <th>population</th>\n",
       "      <th>area_sq_mi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>1787-12-12</td>\n",
       "      <td>12801989</td>\n",
       "      <td>46055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          state date_admitted  population  area_sq_mi\n",
       "0  Pennsylvania    1787-12-12    12801989       46055"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penn_df = pd.DataFrame(penn_info)\n",
    "\n",
    "penn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4YpVRYuNLprf"
   },
   "outputs": [],
   "source": [
    "penn_df.to_csv('Penn_State_Information.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rcz_8faehJEc"
   },
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3TiThTGwMy6Y"
   },
   "source": [
    "**Exercise 4 - Median Household Income**  <br>\n",
    ">Get the median household income for the state of Pennsylvania as a text string and then as an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-DHCVTcTgfJv"
   },
   "outputs": [],
   "source": [
    "mhi_regex = re.compile('household income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y-ZiXWPlhLRA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$59,195[4]'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mhi_str = soup.find(text=mhi_regex).next.next.text\n",
    "mhi_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$59,195', '4]']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mhi_str_split = mhi_str.split('[')\n",
    "mhi_str_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59195"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(mhi_str_split[0].replace(\"$\", \"\").replace(\",\",\"\"))   # keep only 1st element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59195"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mhi = to_int(mhi_str)    # use function to do the same\n",
    "mhi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_JYxx77qPiF5"
   },
   "source": [
    "**Exercise 5 - Median Household Income - Part II**  <br>\n",
    "> Update `state_df` to include median household income. \n",
    "\n",
    "(Hint: One way you can do this: add median household income to `penn_dict` and recreate `state_df`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bGL3lgXRPjKd"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "penn_dict['mhi'] =mhi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YhjQ6efAPjOU"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>date_admitted</th>\n",
       "      <th>population</th>\n",
       "      <th>area_sq_mi</th>\n",
       "      <th>mhi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>1787-12-12</td>\n",
       "      <td>12801989</td>\n",
       "      <td>46055</td>\n",
       "      <td>59195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          state date_admitted  population  area_sq_mi    mhi\n",
       "0  Pennsylvania    1787-12-12    12801989       46055  59195"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penn_info = [penn_dict]\n",
    "penn_df = pd.DataFrame(penn_info)\n",
    "\n",
    "penn_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wFY_mgFbhP1V"
   },
   "source": [
    "# Pipeline Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "re7Tp_7QQ6oQ"
   },
   "source": [
    "Now that we can extract numerical data from this page about Pennsylvania, how would we build out a full analytic or data science project? \n",
    "\n",
    "The next step is to systematically retrieve this information from the Wikipedia page of each US state.  First, let's build reusable functions to find the state's\n",
    "- name\n",
    "- date admitted\n",
    "- population\n",
    "- area\n",
    "- median household income\n",
    "\n",
    "Note: all of this info can be found in the table on the right side of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Hn3k9rWhLTi"
   },
   "outputs": [],
   "source": [
    "def get_name(table):\n",
    "    raw_name = table.find('th').text\n",
    "    return re.match('[A-z\\s]+', raw_name)[0]  \n",
    "\n",
    "def get_date_admitted(table):\n",
    "    raw_date = table.find(text='Admitted to the Union').next.text\n",
    "    return to_date(raw_date)\n",
    "\n",
    "def get_population(table):\n",
    "    raw_population = table.find(text='Population')\\\n",
    "                        .parent.parent.next_sibling\\\n",
    "                        .find('td').text\n",
    "    return to_int(raw_population)\n",
    "\n",
    "def get_area(table):\n",
    "    raw_area = table.find(text=re.compile('Total')).next.text\n",
    "    return to_int(raw_area)\n",
    "\n",
    "def get_income(table):\n",
    "    raw_income = table.find(text='Median household income').next.next.text\n",
    "    return to_int(raw_income)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Vuta_eubJpm"
   },
   "source": [
    "These functions will extract information from any Wikipedia state table we pass into them.  For example, let's try parsing the page for New York."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RnnL6gzcZNCu"
   },
   "outputs": [],
   "source": [
    "ny_url = 'https://en.wikipedia.org/wiki/New_York_(state)'\n",
    "\n",
    "ny_page = requests.get(ny_url).text\n",
    "\n",
    "ny_soup = bs(ny_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9N4UVi7gZNGW"
   },
   "outputs": [],
   "source": [
    "ny_table = ny_soup.find('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B5eDcWr3bzNh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New York'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_name(ny_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gMaiCwZmbzRF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19453561"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_population(ny_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zOunS4nweoMJ"
   },
   "source": [
    "Let's also make a function to gather all five values from a given state Wiki page and return the information as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KEiUBsUveyZJ"
   },
   "outputs": [],
   "source": [
    "def parse_url(url):\n",
    "    page = requests.get(url).text\n",
    "    return bs(page)\n",
    "\n",
    "\n",
    "def get_state_info(state_url):\n",
    "  \n",
    "    #Use parse page and grab main table\n",
    "    state_soup = parse_url(state_url)\n",
    "    state_table = state_soup.find('table')\n",
    "\n",
    "    state_info = {}\n",
    "\n",
    "    #Grab info with pre-defined functions\n",
    "    state_info['state'] = get_name(state_table)\n",
    "    state_info['date_admitted'] = get_date_admitted(state_table)\n",
    "    state_info['population'] = get_population(state_table)\n",
    "    state_info['area'] = get_area(state_table)\n",
    "    state_info['median_household_income'] = get_income(state_table)\n",
    "\n",
    "    return state_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AqaFYVKEguZr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 'New York',\n",
       " 'date_admitted': datetime.datetime(1788, 7, 26, 0, 0),\n",
       " 'population': 19453561,\n",
       " 'area': 54555,\n",
       " 'median_household_income': 64894}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ny_info = get_state_info(ny_url)\n",
    "\n",
    "ny_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lym3M7I7hV8W"
   },
   "source": [
    "### Lists of links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GWRIMd9toIv3"
   },
   "source": [
    "The next step in our process will require us to use our `get_state_info()` function on the URLs of each of the 50 US states.  But how do we know which URLs to visit?  We might be able to guess that the page for Rhode Island is https://en.wikipedia.org/wiki/Rhode_Island but not all pages follow this convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K_Iiv4tphS35"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/New_York_(state)'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ny_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-1Z8jrjcohhI"
   },
   "source": [
    "Instead of guessing, let's first gather these links from this \"[List of States and Territories of the United States](https://en.wikipedia.org/wiki/List_of_states_and_territories_of_the_United_States)\" article. \n",
    "\n",
    "Click on this link and inspect the page to develop a plan for doing this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RsEFM_uCpvXw"
   },
   "source": [
    "It looks like each of the states are listed in the second table of the page.  Each state name and link is contained within table header tags (`th`) and have the additional property of `scope`=\"row\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QtyoJ2IIp8qz"
   },
   "outputs": [],
   "source": [
    "list_url = 'https://en.wikipedia.org/wiki/List_of_states_and_territories_of_the_United_States'\n",
    "list_page = requests.get(list_url).text\n",
    "list_soup = bs(list_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CV44GQo5p8uE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<th scope=\"row\"><span class=\"flagicon\"><img alt=\"\" class=\"thumbborder\" data-file-height=\"400\" data-file-width=\"600\" decoding=\"async\" height=\"15\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/5/5c/Flag_of_Alabama.svg/23px-Flag_of_Alabama.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/5/5c/Flag_of_Alabama.svg/35px-Flag_of_Alabama.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/5/5c/Flag_of_Alabama.svg/45px-Flag_of_Alabama.svg.png 2x\" width=\"23\"/> </span><a href=\"/wiki/Alabama\" title=\"Alabama\">Alabama</a>\n",
       " </th>,\n",
       " <th scope=\"row\"><span class=\"flagicon\"><img alt=\"\" class=\"thumbborder\" data-file-height=\"1000\" data-file-width=\"1416\" decoding=\"async\" height=\"15\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/e/e6/Flag_of_Alaska.svg/21px-Flag_of_Alaska.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/e/e6/Flag_of_Alaska.svg/33px-Flag_of_Alaska.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/e/e6/Flag_of_Alaska.svg/43px-Flag_of_Alaska.svg.png 2x\" width=\"21\"/> </span><a href=\"/wiki/Alaska\" title=\"Alaska\">Alaska</a>\n",
       " </th>,\n",
       " <th scope=\"row\"><span class=\"flagicon\"><img alt=\"\" class=\"thumbborder\" data-file-height=\"600\" data-file-width=\"900\" decoding=\"async\" height=\"15\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/9/9d/Flag_of_Arizona.svg/23px-Flag_of_Arizona.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/9/9d/Flag_of_Arizona.svg/35px-Flag_of_Arizona.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/9/9d/Flag_of_Arizona.svg/45px-Flag_of_Arizona.svg.png 2x\" width=\"23\"/> </span><a href=\"/wiki/Arizona\" title=\"Arizona\">Arizona</a>\n",
       " </th>,\n",
       " <th scope=\"row\"><span class=\"flagicon\"><img alt=\"\" class=\"thumbborder\" data-file-height=\"300\" data-file-width=\"450\" decoding=\"async\" height=\"15\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/9/9d/Flag_of_Arkansas.svg/23px-Flag_of_Arkansas.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/9/9d/Flag_of_Arkansas.svg/35px-Flag_of_Arkansas.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/9/9d/Flag_of_Arkansas.svg/45px-Flag_of_Arkansas.svg.png 2x\" width=\"23\"/> </span><a href=\"/wiki/Arkansas\" title=\"Arkansas\">Arkansas</a>\n",
       " </th>,\n",
       " <th scope=\"row\"><span class=\"flagicon\"><img alt=\"\" class=\"thumbborder\" data-file-height=\"600\" data-file-width=\"900\" decoding=\"async\" height=\"15\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/0/01/Flag_of_California.svg/23px-Flag_of_California.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/0/01/Flag_of_California.svg/35px-Flag_of_California.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/0/01/Flag_of_California.svg/45px-Flag_of_California.svg.png 2x\" width=\"23\"/> </span><a href=\"/wiki/California\" title=\"California\">California</a>\n",
       " </th>]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_rows = list_soup.find_all('table')[0].find_all('th', scope='row')  #Update 5/21/20 Table now first on page, switched from [1]\n",
    "\n",
    "state_rows[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7z6OZyxMsfZ_"
   },
   "source": [
    "Now we just need to extract the links from the `a` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IOMB4AtEsoNz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"/wiki/Alabama\" title=\"Alabama\">Alabama</a>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_rows[0].find('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DEiKQ2nPsoX3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/wiki/Alabama'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_rows[0].find('a')['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-h9Tvy81suE0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/wiki/Alabama',\n",
       " '/wiki/Alaska',\n",
       " '/wiki/Arizona',\n",
       " '/wiki/Arkansas',\n",
       " '/wiki/California']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_links = [row.find('a')['href'] for row in state_rows]\n",
    "\n",
    "state_links[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-bktJb_ds3EY"
   },
   "source": [
    "Each of these links point to a place within Wikipedia, but if we want to link to the full URLs, we have to append 'https://en.wikipedia.org' to each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Neu6J6ps2jn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.wikipedia.org/wiki/Alabama',\n",
       " 'https://en.wikipedia.org/wiki/Alaska',\n",
       " 'https://en.wikipedia.org/wiki/Arizona',\n",
       " 'https://en.wikipedia.org/wiki/Arkansas',\n",
       " 'https://en.wikipedia.org/wiki/California']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = 'https://en.wikipedia.org'\n",
    "\n",
    "state_urls = [base_url + link for link in state_links]\n",
    "\n",
    "state_urls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4N1FOc_2tUP1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.wikipedia.org/wiki/Virginia',\n",
       " 'https://en.wikipedia.org/wiki/Washington_(state)',\n",
       " 'https://en.wikipedia.org/wiki/West_Virginia',\n",
       " 'https://en.wikipedia.org/wiki/Wisconsin',\n",
       " 'https://en.wikipedia.org/wiki/Wyoming']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_urls[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wkEc1QDfuYti"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1sBu14sUhm7_"
   },
   "source": [
    "### Handling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zqMohMqzuLPl"
   },
   "source": [
    "We will eventually be cycling through these state links to collect and store information about every state.  But what happens when certain information is unavailable?  That is, what if the Georgia page is missing area information or the median household income isn't listed for Nevada?\n",
    "\n",
    "We can make our code more robust by including instructions for handling missing information.  One way to do this is to include `try`/`except` statements.\n",
    "\n",
    "If you haven't seen them before, `try`/`except` pairs are used to let Python know how to handle errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1lizdJPIhS7D"
   },
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OGXhFCA8u8xd"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-7afc4bafc47c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hi'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#This returns an error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-90-aaa5847b1481>\u001b[0m in \u001b[0;36msquare\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'str'"
     ]
    }
   ],
   "source": [
    "square('hi')  #This returns an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GvBh3sg7u80g"
   },
   "outputs": [],
   "source": [
    "def square_robust(x):\n",
    "    try:\n",
    "        return x*x\n",
    "    except TypeError:\n",
    "        return \"No can do!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R0i2C7Itu8ui"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No can do!'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "square_robust('hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IEWPFgoovdi8"
   },
   "source": [
    "In the case of web scraping, we may be scraping information from many, many pages.  If any of the information we want can't be found, we usually don't want Python to exit the program with an error.  We typically prefer that Python continue the scraping but just fill in that particular piece of information with a missing value like `None`.\n",
    "\n",
    "```\n",
    "def my_scraper(page):\n",
    "  try:\n",
    "    perform some parsing\n",
    "    return my_scraped_value\n",
    "  except:\n",
    "    return None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KuCDZwOvwOl-"
   },
   "source": [
    "Let's update the collection of our state info to be robust to handling missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PZR8ZeqivQdq"
   },
   "outputs": [],
   "source": [
    "def get_state_info_robust(state_url):\n",
    "  \n",
    "    #If we can't find a main table, \n",
    "    #  print out the url and exit function\n",
    "    try:\n",
    "        state_soup = parse_url(state_url)\n",
    "        state_table = state_soup.find('table')\n",
    "    except:\n",
    "        print(f\"Cannot parse table: {state_url}\")\n",
    "        return None\n",
    "\n",
    "    state_info = {}\n",
    "\n",
    "    #Grab info with pre-defined functions\n",
    "    #  If any value can't be found, just fill value with None\n",
    "    values = ['state', 'date_admitted', 'population', \n",
    "            'area_sq_mi', 'median_household_income']\n",
    "    functions = [get_name, get_date_admitted, get_population,\n",
    "               get_area, get_income]\n",
    "\n",
    "    for val, func in zip(values, functions):\n",
    "        try:\n",
    "            state_info[val] = func(state_table)\n",
    "        except:\n",
    "            state_info[val] = None\n",
    "\n",
    "    return state_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "niZb2eLqv6mB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 'New York',\n",
       " 'date_admitted': datetime.datetime(1788, 7, 26, 0, 0),\n",
       " 'population': 19453561,\n",
       " 'area_sq_mi': 54555,\n",
       " 'median_household_income': 64894}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ny_dict = get_state_info_robust(ny_url)\n",
    "\n",
    "ny_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ny_dict['date_admitted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U1K2dcRkx303"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 'Year\\n',\n",
       " 'date_admitted': None,\n",
       " 'population': None,\n",
       " 'area_sq_mi': None,\n",
       " 'median_household_income': None}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_state_info_robust('https://en.wikipedia.org/wiki/Python_Conference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vHAU0vqgvQhQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot parse table: https://notawebsiteatleastihopenot.net\n"
     ]
    }
   ],
   "source": [
    "get_state_info_robust('https://notawebsiteatleastihopenot.net')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zwz2cjB--lxb"
   },
   "source": [
    "### Adding pauses\n",
    "\n",
    "We have just one final consideration before we cycle through the state links to scrape information.  Web scraping at a fast rate--that is, many pages per second--is frowned upon by many websites, Wikipedia included.  We will add in artificial pauses so we don't overwhelm the Wikipedia server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EsT5a0ZC-qw_"
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OreVacFe2I7d"
   },
   "outputs": [],
   "source": [
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQ_VROSG1mvq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pausing for 5 seconds\n",
      "b equals 6\n"
     ]
    }
   ],
   "source": [
    "a = 5\n",
    "\n",
    "print(f\"Pausing for {a} seconds\")\n",
    "time.sleep(a)\n",
    "\n",
    "b = a + 1\n",
    "print(f\"b equals {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SIp5FtNd2PXW"
   },
   "source": [
    "To responsibly scrape websites, you should know what the site's rate limit is and respect it!  Most sites list their rate limit for web scraping in their `robots.txt` file.  More on this later. \n",
    "\n",
    "Wikipedia requests [at least a one second pause per page request](https://en.wikipedia.org/wiki/Wikipedia:Database_download#Please_do_not_use_a_web_crawler).  We will pause 1 second between each page scrape, so we will only **collect information for 10 US states** for now.\n",
    "\n",
    "**WARNING!!** Not respecting site's limits can get your IP address blocked from the site.  Don't get yourself blocked from Wikipedia!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ju1vGnuhppe"
   },
   "source": [
    "### Data storage revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I_TsxgES21_5"
   },
   "source": [
    "We now have a function to extract information for each state as a dictionary.  We can convert this information into a `pandas` dataframe and store it to an Excel or .csv file if we pass in a list of dictionaries, all with the same keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ePxaqh_MhLWh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'state': 'Pennsylvania',\n",
       "  'date_admitted': datetime.datetime(1787, 12, 12, 0, 0),\n",
       "  'population': 12801989,\n",
       "  'area_sq_mi': 46055,\n",
       "  'mhi': 59195},\n",
       " {'state': 'New York',\n",
       "  'date_admitted': datetime.datetime(1788, 7, 26, 0, 0),\n",
       "  'population': 19453561,\n",
       "  'area_sq_mi': 54555,\n",
       "  'median_household_income': 64894}]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[penn_dict, ny_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MeZUE36g20AJ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>date_admitted</th>\n",
       "      <th>population</th>\n",
       "      <th>area_sq_mi</th>\n",
       "      <th>mhi</th>\n",
       "      <th>median_household_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>1787-12-12</td>\n",
       "      <td>12801989</td>\n",
       "      <td>46055</td>\n",
       "      <td>59195.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New York</td>\n",
       "      <td>1788-07-26</td>\n",
       "      <td>19453561</td>\n",
       "      <td>54555</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64894.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          state date_admitted  population  area_sq_mi      mhi  \\\n",
       "0  Pennsylvania    1787-12-12    12801989       46055  59195.0   \n",
       "1      New York    1788-07-26    19453561       54555      NaN   \n",
       "\n",
       "   median_household_income  \n",
       "0                      NaN  \n",
       "1                  64894.0  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([penn_dict, ny_dict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pGIvrVZm4YYE"
   },
   "source": [
    "Now let's build out our full pipeline:\n",
    "\n",
    "1. Gather a list of links to each state. (DONE)\n",
    "2. For each state link, gather state information as a dictionary.\n",
    "3. Append each state dictionary to a list.\n",
    "4. Convert list of dictionaries to dataframe.\n",
    "5. Save dataframe as a .csv or an Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQbbcPH82z9i"
   },
   "outputs": [],
   "source": [
    "state_info_list = []\n",
    "\n",
    "for link in state_urls[:10]:\n",
    "\n",
    "  #Step 2.\n",
    "  state_info = get_state_info_robust(link)\n",
    "\n",
    "  #Step 3.\n",
    "  if state_info:\n",
    "    state_info_list.append(state_info)\n",
    "\n",
    "  #Be sure to pause\n",
    "  time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y4jne9Wp4wj5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'state': 'Alabama',\n",
       "  'date_admitted': datetime.datetime(1819, 12, 14, 0, 0),\n",
       "  'population': 4903185,\n",
       "  'area_sq_mi': 52419,\n",
       "  'median_household_income': 48123},\n",
       " {'state': 'Alaska',\n",
       "  'date_admitted': datetime.datetime(1959, 1, 3, 0, 0),\n",
       "  'population': 710249,\n",
       "  'area_sq_mi': 663268,\n",
       "  'median_household_income': 73181},\n",
       " {'state': 'Arizona',\n",
       "  'date_admitted': datetime.datetime(1912, 2, 14, 0, 0),\n",
       "  'population': 7278717,\n",
       "  'area_sq_mi': 113990,\n",
       "  'median_household_income': 56581},\n",
       " {'state': 'Arkansas',\n",
       "  'date_admitted': datetime.datetime(1836, 6, 15, 0, 0),\n",
       "  'population': 3017804,\n",
       "  'area_sq_mi': 53179,\n",
       "  'median_household_income': 45869},\n",
       " {'state': 'California',\n",
       "  'date_admitted': datetime.datetime(1850, 9, 9, 0, 0),\n",
       "  'population': 39512223,\n",
       "  'area_sq_mi': 163696,\n",
       "  'median_household_income': 71228},\n",
       " {'state': 'Colorado',\n",
       "  'date_admitted': datetime.datetime(1876, 8, 1, 0, 0),\n",
       "  'population': 5758736,\n",
       "  'area_sq_mi': 104094,\n",
       "  'median_household_income': 69117},\n",
       " {'state': 'Connecticut',\n",
       "  'date_admitted': datetime.datetime(1788, 1, 9, 0, 0),\n",
       "  'population': 3565287,\n",
       "  'area_sq_mi': 5567,\n",
       "  'median_household_income': 76106},\n",
       " {'state': 'Delaware',\n",
       "  'date_admitted': datetime.datetime(1787, 12, 7, 0, 0),\n",
       "  'population': 973764,\n",
       "  'area_sq_mi': 1982,\n",
       "  'median_household_income': 62852},\n",
       " {'state': 'Florida',\n",
       "  'date_admitted': datetime.datetime(1845, 3, 3, 0, 0),\n",
       "  'population': 21477737,\n",
       "  'area_sq_mi': 65757,\n",
       "  'median_household_income': 53267},\n",
       " {'state': 'Georgia',\n",
       "  'date_admitted': datetime.datetime(1788, 1, 2, 0, 0),\n",
       "  'population': 10617423,\n",
       "  'area_sq_mi': 59425,\n",
       "  'median_household_income': 56183}]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KJvOX15v4wot"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>date_admitted</th>\n",
       "      <th>population</th>\n",
       "      <th>area_sq_mi</th>\n",
       "      <th>median_household_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>1819-12-14</td>\n",
       "      <td>4903185</td>\n",
       "      <td>52419</td>\n",
       "      <td>48123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>1959-01-03</td>\n",
       "      <td>710249</td>\n",
       "      <td>663268</td>\n",
       "      <td>73181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>1912-02-14</td>\n",
       "      <td>7278717</td>\n",
       "      <td>113990</td>\n",
       "      <td>56581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>1836-06-15</td>\n",
       "      <td>3017804</td>\n",
       "      <td>53179</td>\n",
       "      <td>45869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>1850-09-09</td>\n",
       "      <td>39512223</td>\n",
       "      <td>163696</td>\n",
       "      <td>71228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Colorado</td>\n",
       "      <td>1876-08-01</td>\n",
       "      <td>5758736</td>\n",
       "      <td>104094</td>\n",
       "      <td>69117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Connecticut</td>\n",
       "      <td>1788-01-09</td>\n",
       "      <td>3565287</td>\n",
       "      <td>5567</td>\n",
       "      <td>76106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Delaware</td>\n",
       "      <td>1787-12-07</td>\n",
       "      <td>973764</td>\n",
       "      <td>1982</td>\n",
       "      <td>62852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Florida</td>\n",
       "      <td>1845-03-03</td>\n",
       "      <td>21477737</td>\n",
       "      <td>65757</td>\n",
       "      <td>53267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Georgia</td>\n",
       "      <td>1788-01-02</td>\n",
       "      <td>10617423</td>\n",
       "      <td>59425</td>\n",
       "      <td>56183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         state date_admitted  population  area_sq_mi  median_household_income\n",
       "0      Alabama    1819-12-14     4903185       52419                    48123\n",
       "1       Alaska    1959-01-03      710249      663268                    73181\n",
       "2      Arizona    1912-02-14     7278717      113990                    56581\n",
       "3     Arkansas    1836-06-15     3017804       53179                    45869\n",
       "4   California    1850-09-09    39512223      163696                    71228\n",
       "5     Colorado    1876-08-01     5758736      104094                    69117\n",
       "6  Connecticut    1788-01-09     3565287        5567                    76106\n",
       "7     Delaware    1787-12-07      973764        1982                    62852\n",
       "8      Florida    1845-03-03    21477737       65757                    53267\n",
       "9      Georgia    1788-01-02    10617423       59425                    56183"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 4.\n",
    "state_data = pd.DataFrame(state_info_list)\n",
    "\n",
    "state_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nd5vSzpX7A4e"
   },
   "outputs": [],
   "source": [
    "#Step 5.\n",
    "state_data.to_csv('state_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1vlhDewR7G3u"
   },
   "outputs": [],
   "source": [
    "files.download('state_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V3O7-pvMcnQ8"
   },
   "source": [
    "Now that you have the state information in .csv format, you can analyze it with any tool you know how to use:\n",
    "- Excel\n",
    "- visualization tools like Tableau\n",
    "- additional Python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lRb0Qr3oZqFu"
   },
   "source": [
    "Note: If you DID gather information for all 50 states, you would find one missing value: Kansas's date admitted.  After visiting/inspecting the Wiki page for [Kansas](https://en.wikipedia.org/wiki/Kansas), do you see why? How could you fix the data extraction function to account for this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b7nCMJYchdjJ"
   },
   "source": [
    "### Systematically named pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e9s1-1k7ooqH"
   },
   "source": [
    "For many web scraping projects, you will begin by collecting links from a links page.  Other times you might be able to devise a pattern in the URLs that you can exploit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YTCHKBFQ2ymV"
   },
   "source": [
    "**Example: Billboard Year-End Hot 100**\n",
    "\n",
    "The _Billboard_ Hot 100 chart is well known for tracking the success of music singles within the US.  At the end of each year, Billboard compiles a list of the top 100 performing songs throughout the year based on the information from Hot 100 charts.  Wikipedia displays this information as an article here: https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_2019\n",
    "\n",
    "How might we compile a list of the most popular song for each year since 2010?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZXpXULZfhji9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_2010\n",
      "https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_2011\n",
      "https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_2012\n",
      "https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_2013\n",
      "https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_2014\n",
      "https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_2015\n",
      "https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_2016\n",
      "https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_2017\n",
      "https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_2018\n",
      "https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_2019\n"
     ]
    }
   ],
   "source": [
    "top_hits = []\n",
    "\n",
    "for year in range(2010, 2020):\n",
    "\n",
    "    #Build URL for each year\n",
    "    base_url = 'https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_'\n",
    "    url = base_url + str(year)\n",
    "    print(url)\n",
    "\n",
    "    page = requests.get(url).text\n",
    "    soup = bs(page)\n",
    "\n",
    "    #Grab top hit text and link\n",
    "    top_hit = soup.find('table', class_='wikitable').find('td')\n",
    "    top_hit_text = top_hit.text\n",
    "    try:\n",
    "        top_hit_link = top_hit.find('a')['href']\n",
    "    except:\n",
    "        top_hit_link = None\n",
    "\n",
    "    #Store results as list of tuples\n",
    "    top_hits.append((year, top_hit_text, top_hit_link))\n",
    "\n",
    "    #Be sure to pause\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aEG-pLW4t-J1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2010, '\"Tik Tok\"', '/wiki/Tik_Tok_(song)'),\n",
       " (2011, '\"Rolling in the Deep\"', '/wiki/Rolling_in_the_Deep'),\n",
       " (2012,\n",
       "  '\"Somebody That I Used to Know\"',\n",
       "  '/wiki/Somebody_That_I_Used_to_Know'),\n",
       " (2013, '\"Thrift Shop\"', '/wiki/Thrift_Shop'),\n",
       " (2014, '\"Happy\"', '/wiki/Happy_(Pharrell_Williams_song)'),\n",
       " (2015, '\"Uptown Funk\"', '/wiki/Uptown_Funk'),\n",
       " (2016, '\"Love Yourself\"', '/wiki/Love_Yourself'),\n",
       " (2017, '\"Shape of You\"', '/wiki/Shape_of_You'),\n",
       " (2018, '\"God\\'s Plan\"', '/wiki/God%27s_Plan_(song)'),\n",
       " (2019, '\"Old Town Road\"', '/wiki/Old_Town_Road')]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "isRTu8yiiH0s"
   },
   "source": [
    "# Mini Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yM1CBfm7iKQB"
   },
   "source": [
    "To practice putting together all of the skills you have learned today, you will be working on your very own mini project.  \n",
    "\n",
    "### Tips\n",
    "Some project advise before getting started:\n",
    "- **Start small and scale up.**  Make sure your code is working on one page before you try to request information from a ton of links.\n",
    "- **Think through data storage before scaling up.** How will you store the information so you can perform analyses on the data you collect?\n",
    "- **Safeguard against missing values.** It is so annoying when a scraping loop breaks on the last link and all other information is lost...\n",
    "- **Pause for 1+ seconds between requests.** Let's try to not get banned from Wikipedia!\n",
    "\n",
    "With those tips in mind, let's get started on a mini project.  Further directions and project ideas can be found here: https://github.com/kimfetti/Conferences/blob/master/PyCon_2020/mini_project_ideas.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PQ5qdEBVf-Mt"
   },
   "source": [
    "---\n",
    "\n",
    "# Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yz-bc4uwU_q3"
   },
   "source": [
    " **Class Exercise - Disambiguation link**  <br>\n",
    ">Let's try to extract this disambiguation link by inspecting the source code and then using `BeautifulSoup`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7yJfk0jwf_OL"
   },
   "outputs": [],
   "source": [
    "soup.find(class_='mw-disambig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aloiqhErVIWl"
   },
   "outputs": [],
   "source": [
    "soup.find(class_='mw-disambig')['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvFWLwEA-FWN"
   },
   "source": [
    " **Exercise 1 - Longitute, Latitude**  <br>\n",
    ">How would you retrieve Pennsylvania's longitude and latitude coordinates from this page?\n",
    "\n",
    "<center>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1N43WC3jwFpV80L7-iLnEZi8aCQZtWcd3\"  alt=\"coordinates\" width=\"600\"/>\n",
    "</center>\n",
    "\n",
    "_Hint: Right click the coordinates directly and then select inspect._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PdAflwzDVLaf"
   },
   "outputs": [],
   "source": [
    "soup.find(class_='geo-dec').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w_igE3BGf5gn"
   },
   "source": [
    " **Exercise 2 - Capital City**  <br>\n",
    ">Write code to extract the capital of Pennsylvania from the main table without using list positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ptQkf-Mi-JwH"
   },
   "outputs": [],
   "source": [
    "soup.find(text='Capital')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YaH8WcFpgSDV"
   },
   "outputs": [],
   "source": [
    "soup.find(text='Capital').next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RmI1_jY1gVdr"
   },
   "outputs": [],
   "source": [
    "soup.find(text='Capital').next.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JtQcBcP4gezo"
   },
   "source": [
    " **Exercise 3 - Reference Links**  <br>\n",
    "> Print out the text of the first three references (at the bottom of the page).  For an added bonus: can you also print all the external links from these three references?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ksaAfA-ogcJ4"
   },
   "outputs": [],
   "source": [
    "references = soup.find(class_='reflist').find_all('li')[:3]\n",
    "\n",
    "for ref in references:\n",
    "    print(ref.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5TiE_XM_gpAU"
   },
   "outputs": [],
   "source": [
    "for ref in references:\n",
    "    for link in ref.find_all('a', class_='external'):\n",
    "        print(link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1P95ywRjN1UR"
   },
   "source": [
    "**Exercise 4 - Median Household Income - Part I**  <br>\n",
    ">Get the median household income for the state of Pennsylvania as a text string and then as an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aizT6cEtgyaA"
   },
   "outputs": [],
   "source": [
    "mhi_text = soup.find(text='Median household income').next.next.text\n",
    "\n",
    "mhi_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D7eeLcmZN5Xv"
   },
   "outputs": [],
   "source": [
    "mhi_int = to_int(mhi_text)\n",
    "\n",
    "mhi_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iECwH9MAO-Ab"
   },
   "source": [
    " **Exercise 5 - Median Household Income - Part II**  <br>\n",
    "> Update `state_df` to include median household income. \n",
    "\n",
    "(Hint: One you can do this: add median household income to `penn_dict` and recreate `state_df`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b-wSVs1GPln9"
   },
   "outputs": [],
   "source": [
    "penn_dict['median_household_income'] = mhi_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-BHdEYN_PpJF"
   },
   "outputs": [],
   "source": [
    "penn_df = pd.DataFrame([penn_dict])\n",
    "\n",
    "penn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qrk6SxP-P57R"
   },
   "outputs": [],
   "source": [
    "# #Alternatively...\n",
    "\n",
    "# state_df['median_household_income'] = [mhi_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CDOVrJxpP6lZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PyCon2020_Scraping_Wikipedia.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
